## Setting

### 1. Conda Env
```bash
conda create -n videoeval python=3.10
conda activate videoeval
```

### 2. pip upgrade
```bash
pip install --upgrade pip
```

### 3. Torch (CUDA 11.8)
```bash
pip install torch==2.7.0 torchvision==0.22.0 torchaudio==2.7.0 --index-url https://download.pytorch.org/whl/cu118
```

> ğŸ”— PyTorch previous version download guide: https://pytorch.org/get-started/previous-versions/

### 4. Verification
```bash
python -c "import torch; print(torch.__version__, torch.cuda.is_available()); print(torch.version.cuda)"
```

### 5. Other Required Packages
```bash
pip install -r requirements.txt
```

## Drive Mini Sample
[Download Drive Mini Sample](https://drive.google.com/drive/folders/1ZZfkhpWVY-U36Y5e62geOWX-euE2JpJx?usp=drive_link)
Original folder is mini-sample/

## Run Captioning
```bash
bash scripts/caption.sh
```

## Bash details
caption.sh..

- `CUDA_VISIBLE_DEVICES`ë¥¼ ì„¤ì •í•´ **ì‚¬ìš©í•  GPU ë²ˆí˜¸**ë¥¼ ì§€ì •í•©ë‹ˆë‹¤.
  - ê¸°ë³¸ê°’: `0,1,2,3`
  - ì‹¤í–‰ ì‹œ í™˜ê²½ë³€ìˆ˜ë¡œ ë®ì–´ì“°ê¸° ê°€ëŠ¥: `CUDA_VISIBLE_DEVICES=1 ./caption.sh`
- ë‚´ë¶€ì—ì„œ `captioning.py`ë¥¼ ì‹¤í–‰í•˜ë©°, í•„ìš”í•œ ì¸ìë¥¼ í•¨ê»˜ ì „ë‹¬í•©ë‹ˆë‹¤.
  - `--model-name`: ì‚¬ìš©í•  VLLM ì²´í¬í¬ì¸íŠ¸(Hugging Face repo)
  - `--input-json-path`: ë¹„ë””ì˜¤ ê²½ë¡œê°€ ë“¤ì–´ìˆëŠ” JSON íŒŒì¼
  - `--output-json-path`: ê²°ê³¼ ìº¡ì…˜ì„ ì €ì¥í•  JSON íŒŒì¼
  - `--use-sys-prompt`, `--sys-prompt`: ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì‚¬ìš© ì—¬ë¶€ ë° ë‚´ìš©
  - `--question-suffix`: ë©”ì¸ query

```bash
#!/bin/bash

# =====================================================================================
# Configurations
# =====================================================================================
# - Set the GPU devices to use. This value can be overridden by setting the environment
#   variable when running the script (e.g., `CUDA_VISIBLE_DEVICES=1 ./caption.sh`).
# - Multiple GPUs can be specified by separating them with commas (e.g., "0,1").
# =====================================================================================
export CUDA_VISIBLE_DEVICES=${CUDA_VISIBLE_DEVICES:-0,1,2,3}

echo "Running captioning with CUDA_VISIBLE_DEVICES=${CUDA_VISIBLE_DEVICES}"

# Run the python script, passing all command-line arguments to it.
#
# Example usage:
# ./caption.sh \
#   --input-json-path "example_video_paths.json" \
#   --output-json-path "results.json" \
#   --model-name "OpenGVLab/InternVL3_5-8B"
#
python captioning.py \
   --model-name "OpenGVLab/InternVL3_5-38B" \
   --input-json-path input_video_paths.json \
   --output-json-path output_captions.json \
   --use-sys-prompt False \
   --sys-prompt "You are an AI assistant that rigorously follows this response protocol: 1. First, conduct a detailed analysis of the question. Consider different angles, potential solutions, and reason through the problem step-by-step. Enclose this entire thinking process within <think> and </think> tags. 2. After the thinking section, provide a clear, concise, and direct answer to the user's question. Separate the answer from the think section with a newline. Ensure that the thinking process is thorough but remains focused on the query. The final answer should be standalone and not reference the thinking section." \
   --question-suffix "Please provide detailed and comprehensive captions for the video."
```
